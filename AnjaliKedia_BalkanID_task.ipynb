{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UJSkuKY2WFy",
        "outputId": "4ab707ba-2ed2-487e-93a6-8711383dd85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.2.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910454 sha256=9f2d415b3e1823e59f2bb0d3a734d29ef976fcba1d44527ef2ba164a26b9e9db\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "pip install torch-geometric\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "# Load the CSV data\n",
        "df = pd.read_csv('/content/updated_authors.csv')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_PDhz2A78BUZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "# import pandas as pd\n",
        "\n",
        "\n",
        "def extract_author_id(author_instance):\n",
        "    match = re.search(r'author_id: \"([^\"]+)\"', author_instance)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    else:\n",
        "        return None  # Return None if no match is found\n",
        "\n",
        "# Apply the extract_author_id function to the 'a' column and create a new column 'author_id'\n",
        "df['author_id'] = df['a'].apply(extract_author_id)\n",
        "\n",
        "# Print the DataFrame with extracted author_id values\n",
        "print(df['author_id'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC5rrUiSfbol",
        "outputId": "d0ff0914-045b-4f37-ab54-d9c4bb0ae786"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      authorID_9a049_b03f6_fc40b_fcf2f_13632\n",
            "1      authorID_1be00_34108_2e25c_4e251_ca671\n",
            "2      authorID_c2356_069e9_d1e79_ca924_37815\n",
            "3      authorID_3635a_91e3d_a857f_7847f_68185\n",
            "4      authorID_cba28_b89eb_85949_7f544_956d6\n",
            "                        ...                  \n",
            "342    authorID_d29d5_3701d_3c859_e29e1_b9002\n",
            "343    authorID_d8658_0a57f_7bf54_2e852_02283\n",
            "344    authorID_d7cda_a5ca0_58207_6c8e7_72cce\n",
            "345    authorID_6db6e_b4af1_e18ab_81d38_78e44\n",
            "346    authorID_7acc6_84a84_8a9b9_54959_fdd22\n",
            "Name: author_id, Length: 347, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the author for which you want to predict co-authors (Author_X)\n",
        "# author_x = 'authorID_766cb_53c75_3baed_ac5dc_78259'\n",
        "index_to_select = 100  # Change this to the desired row number\n",
        "\n",
        "# Get the author_id from the selected row\n",
        "author_x = df.loc[index_to_select, 'author_id']\n",
        "# Create a label column based on whether each row's 'author_id' matches Author_X\n",
        "df['label'] = (df['author_id'] == author_x).astype(int)\n",
        "labels = df['label'].tolist()\n",
        "\n",
        "# Now, the 'label' column contains binary labels (1 for Author_X, 0 for others)"
      ],
      "metadata": {
        "id": "H9nsmKRuh4Wr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Graph Representation:\n",
        "You'll need to construct a graph representation from the 'a' and 'b' columns of your DataFrame. In this case, 'a' and 'b' represent co-author relationships. You can create an edge list from this data."
      ],
      "metadata": {
        "id": "i0iDeZbG-W3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_authors = df['a'].unique().tolist() + df['coauthors'].unique().tolist()\n",
        "\n",
        "# Create a dictionary to map author IDs to unique numerical indices\n",
        "author_id_to_index = {author_id: index for index, author_id in enumerate(all_authors)}\n",
        "\n",
        "# Map 'a' and 'b' columns to numerical indices\n",
        "df['a_numeric'] = df['a'].map(author_id_to_index)\n",
        "df['b_numeric'] = df['coauthors'].map(author_id_to_index)\n",
        "\n",
        "# Extract the numerical edge list\n",
        "edge_list = df[['a_numeric', 'b_numeric']].values.tolist()\n",
        "\n",
        "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(edge_index=edge_index)\n",
        "data.y = labels\n",
        "\n",
        "# Create a placeholder tensor for node features\n",
        "num_nodes = len(all_authors)  # Number of nodes in the graph\n",
        "num_features = 64  # Number of features (adjust as needed)\n",
        "data.x = torch.randn(num_nodes, num_features)  # Placeholder node features\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V0Kz5FRy9cMZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a NetworkX Graph:\n",
        "You can convert the edge list into a NetworkX graph, which is a common format for working with graphs in Python. NetworkX can be used to perform various graph operations."
      ],
      "metadata": {
        "id": "O1iNdJDr-aDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a NetworkX graph from the edge list\n",
        "G = nx.Graph()\n",
        "G.add_edges_from(edge_list)\n"
      ],
      "metadata": {
        "id": "ijCHrqXI-brm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GNNModel is a custom GNN model class that inherits from nn.Module.\n",
        "The model consists of two GCN layers (self.conv1 and self.conv2), but you can adjust the number of layers and hidden units as needed.\n",
        "The forward method defines the forward pass of the model. It applies the first GCN layer, a ReLU activation, and then the second GCN layer."
      ],
      "metadata": {
        "id": "WtmaQWvt_sMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, num_nodes, num_features, num_classes):\n",
        "        super(GNNModel, self).__init__()\n",
        "        self.conv1 = GCNConv(num_features, 64)  # GCN layer with 64 output channels\n",
        "        self.conv2 = GCNConv(64, num_classes)   # GCN layer with output size equal to the number of classes\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        # Apply the first GCN layer followed by a ReLU activation\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Apply the second GCN layer\n",
        "        x = self.conv2(x, edge_index)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the GNN model\n",
        "num_nodes = len(all_authors)  # The total number of authors in  dataset\n",
        "num_features = 64\n",
        "num_classes = 1  # Adjust to the number of classes in your classification task\n",
        "model = GNNModel(num_nodes, num_features, num_classes)\n",
        "\n"
      ],
      "metadata": {
        "id": "2gTWEE_x_YCV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this modified code, we use the length of data.edge_index[0] to determine the number of nodes since the edge index implicitly defines the nodes. We then generate synthetic labels for authors and proceed with the data split.\n",
        "\n",
        "This approach assumes that you don't have node features, and your GNN will operate solely based on the graph structure (edge information)."
      ],
      "metadata": {
        "id": "Q-PYtUAHBW4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define Loss Function and Optimizer:\n",
        "\n",
        "You need to choose an appropriate loss function and optimizer for your specific task. For example, if you are performing binary classification, you can use binary cross-entropy loss and the Adam optimizer."
      ],
      "metadata": {
        "id": "G1E40YNwBNNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss for binary classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate\n"
      ],
      "metadata": {
        "id": "lu6ZxTahBONh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the number of nodes in your graph (based on data or labels)\n",
        "num_nodes = len(data.y)  # Assuming labels are correctly aligned\n",
        "\n",
        "# Split your data into training, validation, and test sets\n",
        "\n",
        "import random\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Create a list of indices for all nodes\n",
        "num_nodes = len(data.y)\n",
        "all_indices = list(range(num_nodes))\n",
        "\n",
        "# Determine the number of nodes for each split\n",
        "train_size = int(train_ratio * num_nodes)\n",
        "val_size = int(val_ratio * num_nodes)\n",
        "test_size = num_nodes - train_size - val_size\n",
        "\n",
        "# Ensure that train_size does not exceed the maximum valid index\n",
        "train_size = min(train_size, num_nodes - 1)\n",
        "\n",
        "# Shuffle the indices randomly\n",
        "random.shuffle(all_indices[:train_size])\n",
        "\n",
        "# Split the indices into train, validation, and test sets\n",
        "train_indices = all_indices[:train_size]\n",
        "val_indices = all_indices[train_size:train_size + val_size]\n",
        "test_indices = all_indices[train_size + val_size:]\n",
        "\n",
        "# Extract the labels for each set\n",
        "train_labels = [labels[i] for i in train_indices]\n",
        "val_labels = [labels[i] for i in val_indices]\n",
        "test_labels = [labels[i] for i in test_indices]\n"
      ],
      "metadata": {
        "id": "XJ9Xru_EJDEu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_indices))\n",
        "print(len(train_labels))\n",
        "print(num_nodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qAbAz94tLRh",
        "outputId": "bd4b05ba-ad85-4cae-c90e-8bf47e6aa9cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "277\n",
            "277\n",
            "347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Debugging: Print the maximum index in train_indices\n",
        "max_index = max(train_indices)\n",
        "print(\"Max Index in train_indices:\", max_index)\n",
        "\n",
        "# Print the number of nodes\n",
        "print(\"Number of Nodes in Dataset:\", num_nodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1PJMDkLukc9",
        "outputId": "6fd61f34-1177-4a9a-cdfa-c972fde88cb0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Index in train_indices: 276\n",
            "Number of Nodes in Dataset: 347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "train_labels_tensor = torch.zeros((data.size(0),))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(data)\n",
        "    output = torch.squeeze(output)\n",
        "\n",
        "\n",
        "\n",
        "    # Print a subset of output and train_labels_tensor for debugging\n",
        "    print(\"Output:\", output[:10])  # Print the first 10 elements\n",
        "    print(\"Train labels:\", train_labels_tensor[:10])  # Print the first 10 labels\n",
        "\n",
        "    # Ensure that train_labels_tensor matches the size of output\n",
        "    train_labels_tensor = train_labels_tensor.view(-1)\n",
        "\n",
        "    # Debugging: Print a subset of train_indices\n",
        "    # print(\"Train indices:\", train_indices[:10])  # Print the first 10 indices\n",
        "\n",
        "    # Ensure that train_indices are within bounds\n",
        "    train_indices = [idx for idx in train_indices if idx < output.size(0)]\n",
        "\n",
        "    # Calculate the binary cross-entropy loss using train_labels_tensor\n",
        "    train_loss = criterion(output[train_indices], train_labels_tensor[train_indices])\n",
        "\n",
        "    # Backpropagation\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print loss for monitoring\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss.item()}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he8n_jmpvS9M",
        "outputId": "f39464d8-c9c8-437d-836e-736b2508b8b6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([ 1.7591,  1.0912,  2.2726, -1.9933,  2.0105,  0.5205, -1.0657,  1.2347,\n",
            "        -0.2799,  0.7778], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [1/100], Training Loss: 1.2398618459701538\n",
            "Output: tensor([ 1.6680,  1.0309,  2.1955, -2.0781,  1.9436,  0.4658, -1.0899,  1.1793,\n",
            "        -0.3546,  0.7258], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [2/100], Training Loss: 1.1954072713851929\n",
            "Output: tensor([ 1.5769,  0.9716,  2.1201, -2.1635,  1.8755,  0.4105, -1.1155,  1.1241,\n",
            "        -0.4285,  0.6745], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [3/100], Training Loss: 1.1519497632980347\n",
            "Output: tensor([ 1.4873,  0.9127,  2.0457, -2.2471,  1.8065,  0.3556, -1.1426,  1.0680,\n",
            "        -0.5023,  0.6236], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [4/100], Training Loss: 1.1095293760299683\n",
            "Output: tensor([ 1.3988,  0.8541,  1.9726, -2.3285,  1.7376,  0.3011, -1.1689,  1.0122,\n",
            "        -0.5773,  0.5729], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [5/100], Training Loss: 1.068172574043274\n",
            "Output: tensor([ 1.3108,  0.7957,  1.8996, -2.4078,  1.6700,  0.2467, -1.1947,  0.9563,\n",
            "        -0.6526,  0.5223], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [6/100], Training Loss: 1.0278856754302979\n",
            "Output: tensor([ 1.2255,  0.7370,  1.8268, -2.4862,  1.6022,  0.1919, -1.2207,  0.9006,\n",
            "        -0.7285,  0.4715], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [7/100], Training Loss: 0.9887288808822632\n",
            "Output: tensor([ 1.1435,  0.6783,  1.7542, -2.5634,  1.5343,  0.1371, -1.2464,  0.8453,\n",
            "        -0.8040,  0.4212], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [8/100], Training Loss: 0.9506974816322327\n",
            "Output: tensor([ 1.0622,  0.6198,  1.6817, -2.6393,  1.4663,  0.0825, -1.2725,  0.7903,\n",
            "        -0.8792,  0.3718], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [9/100], Training Loss: 0.9137787818908691\n",
            "Output: tensor([ 0.9816,  0.5611,  1.6097, -2.7138,  1.3978,  0.0276, -1.2988,  0.7355,\n",
            "        -0.9539,  0.3227], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [10/100], Training Loss: 0.8779973983764648\n",
            "Output: tensor([ 0.9018,  0.5028,  1.5382, -2.7867,  1.3297, -0.0273, -1.3252,  0.6810,\n",
            "        -1.0279,  0.2738], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [11/100], Training Loss: 0.8434027433395386\n",
            "Output: tensor([ 0.8227,  0.4448,  1.4668, -2.8579,  1.2645, -0.0823, -1.3518,  0.6265,\n",
            "        -1.1011,  0.2249], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [12/100], Training Loss: 0.8099905252456665\n",
            "Output: tensor([ 0.7443,  0.3879,  1.3953, -2.9272,  1.2000, -0.1375, -1.3784,  0.5724,\n",
            "        -1.1733,  0.1779], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [13/100], Training Loss: 0.7777033448219299\n",
            "Output: tensor([ 0.6666,  0.3325,  1.3240, -2.9946,  1.1375, -0.1925, -1.4050,  0.5184,\n",
            "        -1.2443,  0.1315], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [14/100], Training Loss: 0.7465587854385376\n",
            "Output: tensor([ 0.5894,  0.2782,  1.2537, -3.0604,  1.0755, -0.2475, -1.4317,  0.4649,\n",
            "        -1.3139,  0.0853], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [15/100], Training Loss: 0.7165320515632629\n",
            "Output: tensor([ 0.5130,  0.2244,  1.1835, -3.1248,  1.0140, -0.3026, -1.4581,  0.4118,\n",
            "        -1.3820,  0.0391], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [16/100], Training Loss: 0.687645435333252\n",
            "Output: tensor([ 0.4371,  0.1709,  1.1133, -3.1884,  0.9529, -0.3575, -1.4843,  0.3592,\n",
            "        -1.4485, -0.0071], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [17/100], Training Loss: 0.659881055355072\n",
            "Output: tensor([ 0.3603,  0.1179,  1.0431, -3.2496,  0.8924, -0.4123, -1.5104,  0.3072,\n",
            "        -1.5136, -0.0535], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [18/100], Training Loss: 0.6332100033760071\n",
            "Output: tensor([ 0.2830,  0.0655,  0.9731, -3.3088,  0.8325, -0.4670, -1.5365,  0.2560,\n",
            "        -1.5772, -0.0999], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [19/100], Training Loss: 0.6076043248176575\n",
            "Output: tensor([ 0.2087,  0.0140,  0.9027, -3.3657,  0.7731, -0.5215, -1.5623,  0.2054,\n",
            "        -1.6396, -0.1460], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [20/100], Training Loss: 0.583045244216919\n",
            "Output: tensor([ 0.1352, -0.0367,  0.8320, -3.4207,  0.7144, -0.5757, -1.5870,  0.1554,\n",
            "        -1.7008, -0.1918], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [21/100], Training Loss: 0.5595101714134216\n",
            "Output: tensor([ 0.0624, -0.0867,  0.7613, -3.4738,  0.6563, -0.6296, -1.6119,  0.1062,\n",
            "        -1.7607, -0.2373], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [22/100], Training Loss: 0.5369714498519897\n",
            "Output: tensor([-0.0097, -0.1360,  0.6906, -3.5253,  0.5985, -0.6832, -1.6369,  0.0577,\n",
            "        -1.8193, -0.2823], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [23/100], Training Loss: 0.5154096484184265\n",
            "Output: tensor([-0.0812, -0.1847,  0.6199, -3.5749,  0.5413, -0.7366, -1.6618,  0.0100,\n",
            "        -1.8761, -0.3268], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [24/100], Training Loss: 0.49480491876602173\n",
            "Output: tensor([-0.1522, -0.2331,  0.5496, -3.6252,  0.4856, -0.7896, -1.6868, -0.0370,\n",
            "        -1.9298, -0.3711], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [25/100], Training Loss: 0.4751090109348297\n",
            "Output: tensor([-0.2232, -0.2810,  0.4799, -3.6751,  0.4305, -0.8423, -1.7116, -0.0832,\n",
            "        -1.9824, -0.4149], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [26/100], Training Loss: 0.4562779366970062\n",
            "Output: tensor([-0.2936, -0.3268,  0.4108, -3.7234,  0.3756, -0.8947, -1.7358, -0.1289,\n",
            "        -2.0339, -0.4589], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [27/100], Training Loss: 0.43828821182250977\n",
            "Output: tensor([-0.3630, -0.3718,  0.3422, -3.7702,  0.3210, -0.9467, -1.7595, -0.1741,\n",
            "        -2.0842, -0.5031], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [28/100], Training Loss: 0.42110946774482727\n",
            "Output: tensor([-0.4315, -0.4159,  0.2743, -3.8158,  0.2669, -0.9980, -1.7829, -0.2189,\n",
            "        -2.1335, -0.5468], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [29/100], Training Loss: 0.4047015607357025\n",
            "Output: tensor([-0.4990, -0.4593,  0.2073, -3.8601,  0.2131, -1.0487, -1.8065, -0.2633,\n",
            "        -2.1818, -0.5903], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [30/100], Training Loss: 0.38904327154159546\n",
            "Output: tensor([-0.5654, -0.5021,  0.1414, -3.9032,  0.1596, -1.0994, -1.8303, -0.3072,\n",
            "        -2.2292, -0.6333], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [31/100], Training Loss: 0.3741258680820465\n",
            "Output: tensor([-0.6305, -0.5442,  0.0766, -3.9452,  0.1064, -1.1497, -1.8541, -0.3506,\n",
            "        -2.2756, -0.6758], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [32/100], Training Loss: 0.35990676283836365\n",
            "Output: tensor([-0.6944, -0.5856,  0.0131, -3.9860,  0.0538, -1.1993, -1.8780, -0.3936,\n",
            "        -2.3212, -0.7180], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [33/100], Training Loss: 0.34635475277900696\n",
            "Output: tensor([-7.5733e-01, -6.2645e-01, -4.9030e-02, -4.0259e+00,  1.6312e-03,\n",
            "        -1.2484e+00, -1.9020e+00, -4.3605e-01, -2.3660e+00, -7.5956e-01],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [34/100], Training Loss: 0.33344122767448425\n",
            "Output: tensor([-0.8190, -0.6665, -0.1098, -4.0648, -0.0500, -1.2969, -1.9260, -0.4780,\n",
            "        -2.4101, -0.8005], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [35/100], Training Loss: 0.32113686203956604\n",
            "Output: tensor([-0.8792, -0.7059, -0.1691, -4.1029, -0.1010, -1.3448, -1.9502, -0.5194,\n",
            "        -2.4533, -0.8410], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [36/100], Training Loss: 0.30941665172576904\n",
            "Output: tensor([-0.9380, -0.7445, -0.2267, -4.1402, -0.1514, -1.3921, -1.9745, -0.5602,\n",
            "        -2.4958, -0.8811], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [37/100], Training Loss: 0.2982468605041504\n",
            "Output: tensor([-0.9953, -0.7824, -0.2824, -4.1767, -0.2012, -1.4388, -1.9990, -0.6006,\n",
            "        -2.5377, -0.9207], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [38/100], Training Loss: 0.2875984013080597\n",
            "Output: tensor([-1.0513, -0.8198, -0.3358, -4.2124, -0.2505, -1.4849, -2.0235, -0.6403,\n",
            "        -2.5789, -0.9599], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [39/100], Training Loss: 0.2774546444416046\n",
            "Output: tensor([-1.1059, -0.8565, -0.3879, -4.2475, -0.2992, -1.5304, -2.0479, -0.6794,\n",
            "        -2.6195, -0.9981], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [40/100], Training Loss: 0.26779234409332275\n",
            "Output: tensor([-1.1593, -0.8925, -0.4385, -4.2821, -0.3471, -1.5753, -2.0723, -0.7175,\n",
            "        -2.6594, -1.0357], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [41/100], Training Loss: 0.2585891783237457\n",
            "Output: tensor([-1.2108, -0.9280, -0.4880, -4.3162, -0.3943, -1.6196, -2.0957, -0.7535,\n",
            "        -2.6987, -1.0730], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [42/100], Training Loss: 0.2498256266117096\n",
            "Output: tensor([-1.2609, -0.9629, -0.5362, -4.3498, -0.4405, -1.6635, -2.1194, -0.7892,\n",
            "        -2.7376, -1.1080], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [43/100], Training Loss: 0.24147692322731018\n",
            "Output: tensor([-1.3099, -0.9973, -0.5832, -4.3828, -0.4859, -1.7068, -2.1431, -0.8244,\n",
            "        -2.7758, -1.1422], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [44/100], Training Loss: 0.2335159331560135\n",
            "Output: tensor([-1.3577, -1.0310, -0.6290, -4.4154, -0.5298, -1.7497, -2.1669, -0.8591,\n",
            "        -2.8135, -1.1760], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [45/100], Training Loss: 0.22592560946941376\n",
            "Output: tensor([-1.4044, -1.0642, -0.6735, -4.4474, -0.5731, -1.7920, -2.1907, -0.8935,\n",
            "        -2.8504, -1.2093], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [46/100], Training Loss: 0.21868440508842468\n",
            "Output: tensor([-1.4500, -1.0969, -0.7170, -4.4790, -0.6158, -1.8337, -2.2145, -0.9273,\n",
            "        -2.8866, -1.2422], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [47/100], Training Loss: 0.21177153289318085\n",
            "Output: tensor([-1.4947, -1.1291, -0.7594, -4.5098, -0.6577, -1.8750, -2.2382, -0.9608,\n",
            "        -2.9220, -1.2748], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [48/100], Training Loss: 0.20516791939735413\n",
            "Output: tensor([-1.5383, -1.1608, -0.8008, -4.5400, -0.6987, -1.9153, -2.2619, -0.9939,\n",
            "        -2.9567, -1.3070], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [49/100], Training Loss: 0.19885987043380737\n",
            "Output: tensor([-1.5810, -1.1921, -0.8413, -4.5696, -0.7390, -1.9547, -2.2856, -1.0267,\n",
            "        -2.9906, -1.3388], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [50/100], Training Loss: 0.19283123314380646\n",
            "Output: tensor([-1.6226, -1.2229, -0.8809, -4.5987, -0.7788, -1.9929, -2.3092, -1.0591,\n",
            "        -3.0240, -1.3702], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [51/100], Training Loss: 0.1870678961277008\n",
            "Output: tensor([-1.6634, -1.2532, -0.9195, -4.6273, -0.8179, -2.0305, -2.3328, -1.0912,\n",
            "        -3.0566, -1.4011], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [52/100], Training Loss: 0.18155592679977417\n",
            "Output: tensor([-1.7033, -1.2830, -0.9573, -4.6552, -0.8565, -2.0677, -2.3562, -1.1230,\n",
            "        -3.0885, -1.4317], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [53/100], Training Loss: 0.17627812922000885\n",
            "Output: tensor([-1.7423, -1.3123, -0.9941, -4.6825, -0.8945, -2.1044, -2.3794, -1.1545,\n",
            "        -3.1197, -1.4618], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [54/100], Training Loss: 0.17122609913349152\n",
            "Output: tensor([-1.7804, -1.3410, -1.0301, -4.7092, -0.9318, -2.1407, -2.4023, -1.1856,\n",
            "        -3.1501, -1.4915], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [55/100], Training Loss: 0.16638833284378052\n",
            "Output: tensor([-1.8176, -1.3692, -1.0660, -4.7353, -0.9684, -2.1765, -2.4247, -1.2165,\n",
            "        -3.1797, -1.5207], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [56/100], Training Loss: 0.161753311753273\n",
            "Output: tensor([-1.8541, -1.3967, -1.1023, -4.7605, -1.0045, -2.2120, -2.4468, -1.2469,\n",
            "        -3.2085, -1.5495], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [57/100], Training Loss: 0.15730760991573334\n",
            "Output: tensor([-1.8896, -1.4239, -1.1378, -4.7851, -1.0398, -2.2471, -2.4687, -1.2768,\n",
            "        -3.2366, -1.5779], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [58/100], Training Loss: 0.1530449539422989\n",
            "Output: tensor([-1.9242, -1.4505, -1.1726, -4.8089, -1.0745, -2.2817, -2.4903, -1.3064,\n",
            "        -3.2640, -1.6059], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [59/100], Training Loss: 0.14895763993263245\n",
            "Output: tensor([-1.9582, -1.4766, -1.2073, -4.8322, -1.1084, -2.3161, -2.5117, -1.3354,\n",
            "        -3.2906, -1.6334], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [60/100], Training Loss: 0.14503467082977295\n",
            "Output: tensor([-1.9914, -1.5021, -1.2419, -4.8548, -1.1418, -2.3501, -2.5328, -1.3640,\n",
            "        -3.3166, -1.6607], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [61/100], Training Loss: 0.14126862585544586\n",
            "Output: tensor([-2.0241, -1.5271, -1.2758, -4.8769, -1.1746, -2.3835, -2.5536, -1.3921,\n",
            "        -3.3420, -1.6875], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [62/100], Training Loss: 0.13765160739421844\n",
            "Output: tensor([-2.0561, -1.5517, -1.3090, -4.8984, -1.2068, -2.4166, -2.5742, -1.4197,\n",
            "        -3.3670, -1.7141], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [63/100], Training Loss: 0.13417582213878632\n",
            "Output: tensor([-2.0875, -1.5758, -1.3417, -4.9194, -1.2384, -2.4492, -2.5945, -1.4468,\n",
            "        -3.3914, -1.7407], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [64/100], Training Loss: 0.1308349370956421\n",
            "Output: tensor([-2.1183, -1.5995, -1.3737, -4.9399, -1.2695, -2.4814, -2.6147, -1.4735,\n",
            "        -3.4153, -1.7670], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [65/100], Training Loss: 0.12762117385864258\n",
            "Output: tensor([-2.1484, -1.6229, -1.4051, -4.9600, -1.3000, -2.5131, -2.6345, -1.4997,\n",
            "        -3.4389, -1.7931], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [66/100], Training Loss: 0.12453030794858932\n",
            "Output: tensor([-2.1779, -1.6460, -1.4359, -4.9798, -1.3299, -2.5444, -2.6541, -1.5254,\n",
            "        -3.4620, -1.8188], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [67/100], Training Loss: 0.12155667692422867\n",
            "Output: tensor([-2.2068, -1.6687, -1.4662, -4.9995, -1.3593, -2.5752, -2.6734, -1.5506,\n",
            "        -3.4846, -1.8442], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [68/100], Training Loss: 0.1186913475394249\n",
            "Output: tensor([-2.2351, -1.6910, -1.4959, -5.0189, -1.3882, -2.6056, -2.6923, -1.5753,\n",
            "        -3.5069, -1.8693], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [69/100], Training Loss: 0.11593017727136612\n",
            "Output: tensor([-2.2629, -1.7131, -1.5251, -5.0382, -1.4166, -2.6356, -2.7110, -1.5997,\n",
            "        -3.5287, -1.8941], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [70/100], Training Loss: 0.11326909065246582\n",
            "Output: tensor([-2.2903, -1.7349, -1.5537, -5.0572, -1.4445, -2.6652, -2.7296, -1.6236,\n",
            "        -3.5502, -1.9185], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [71/100], Training Loss: 0.11070267111063004\n",
            "Output: tensor([-2.3171, -1.7563, -1.5818, -5.0762, -1.4720, -2.6942, -2.7479, -1.6471,\n",
            "        -3.5713, -1.9426], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [72/100], Training Loss: 0.10822717100381851\n",
            "Output: tensor([-2.3435, -1.7773, -1.6094, -5.0950, -1.4990, -2.7229, -2.7661, -1.6703,\n",
            "        -3.5921, -1.9665], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [73/100], Training Loss: 0.10583844035863876\n",
            "Output: tensor([-2.3694, -1.7980, -1.6364, -5.1135, -1.5254, -2.7512, -2.7840, -1.6933,\n",
            "        -3.6125, -1.9900], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [74/100], Training Loss: 0.10353212803602219\n",
            "Output: tensor([-2.3948, -1.8184, -1.6630, -5.1318, -1.5514, -2.7790, -2.8018, -1.7159,\n",
            "        -3.6326, -2.0134], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [75/100], Training Loss: 0.1013045459985733\n",
            "Output: tensor([-2.4198, -1.8386, -1.6890, -5.1498, -1.5771, -2.8063, -2.8196, -1.7382,\n",
            "        -3.6524, -2.0365], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [76/100], Training Loss: 0.09915220737457275\n",
            "Output: tensor([-2.4445, -1.8584, -1.7146, -5.1676, -1.6024, -2.8333, -2.8372, -1.7601,\n",
            "        -3.6718, -2.0594], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [77/100], Training Loss: 0.09707130491733551\n",
            "Output: tensor([-2.4688, -1.8780, -1.7399, -5.1851, -1.6273, -2.8599, -2.8547, -1.7818,\n",
            "        -3.6910, -2.0820], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [78/100], Training Loss: 0.09505809843540192\n",
            "Output: tensor([-2.4928, -1.8973, -1.7647, -5.2024, -1.6519, -2.8860, -2.8721, -1.8033,\n",
            "        -3.7098, -2.1044], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [79/100], Training Loss: 0.09311038255691528\n",
            "Output: tensor([-2.5164, -1.9164, -1.7891, -5.2194, -1.6762, -2.9117, -2.8894, -1.8244,\n",
            "        -3.7283, -2.1265], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [80/100], Training Loss: 0.09122607856988907\n",
            "Output: tensor([-2.5397, -1.9353, -1.8132, -5.2362, -1.7001, -2.9375, -2.9065, -1.8452,\n",
            "        -3.7466, -2.1485], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [81/100], Training Loss: 0.08940238505601883\n",
            "Output: tensor([-2.5627, -1.9539, -1.8367, -5.2529, -1.7237, -2.9630, -2.9235, -1.8659,\n",
            "        -3.7647, -2.1701], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [82/100], Training Loss: 0.08763647079467773\n",
            "Output: tensor([-2.5853, -1.9724, -1.8598, -5.2693, -1.7470, -2.9881, -2.9404, -1.8863,\n",
            "        -3.7824, -2.1916], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [83/100], Training Loss: 0.08592547476291656\n",
            "Output: tensor([-2.6076, -1.9906, -1.8826, -5.2855, -1.7700, -3.0128, -2.9571, -1.9064,\n",
            "        -3.7998, -2.2128], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [84/100], Training Loss: 0.08426740020513535\n",
            "Output: tensor([-2.6296, -2.0086, -1.9051, -5.3015, -1.7927, -3.0372, -2.9737, -1.9264,\n",
            "        -3.8169, -2.2338], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [85/100], Training Loss: 0.08265993744134903\n",
            "Output: tensor([-2.6513, -2.0265, -1.9273, -5.3173, -1.8151, -3.0613, -2.9902, -1.9462,\n",
            "        -3.8338, -2.2545], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [86/100], Training Loss: 0.08110091835260391\n",
            "Output: tensor([-2.6729, -2.0443, -1.9492, -5.3329, -1.8373, -3.0851, -3.0065, -1.9657,\n",
            "        -3.8504, -2.2749], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [87/100], Training Loss: 0.07958823442459106\n",
            "Output: tensor([-2.6945, -2.0620, -1.9707, -5.3485, -1.8592, -3.1084, -3.0228, -1.9851,\n",
            "        -3.8669, -2.2952], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [88/100], Training Loss: 0.07811997830867767\n",
            "Output: tensor([-2.7160, -2.0796, -1.9917, -5.3639, -1.8808, -3.1315, -3.0390, -2.0043,\n",
            "        -3.8832, -2.3153], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [89/100], Training Loss: 0.07669435441493988\n",
            "Output: tensor([-2.7373, -2.0970, -2.0125, -5.3791, -1.9022, -3.1543, -3.0551, -2.0233,\n",
            "        -3.8993, -2.3352], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [90/100], Training Loss: 0.07530907541513443\n",
            "Output: tensor([-2.7584, -2.1143, -2.0330, -5.3941, -1.9233, -3.1768, -3.0712, -2.0421,\n",
            "        -3.9152, -2.3550], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [91/100], Training Loss: 0.07396287471055984\n",
            "Output: tensor([-2.7794, -2.1314, -2.0532, -5.4089, -1.9442, -3.1990, -3.0871, -2.0606,\n",
            "        -3.9311, -2.3746], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [92/100], Training Loss: 0.07265400886535645\n",
            "Output: tensor([-2.8001, -2.1482, -2.0732, -5.4235, -1.9648, -3.2209, -3.1030, -2.0789,\n",
            "        -3.9468, -2.3941], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [93/100], Training Loss: 0.07138156145811081\n",
            "Output: tensor([-2.8206, -2.1648, -2.0929, -5.4382, -1.9852, -3.2425, -3.1187, -2.0971,\n",
            "        -3.9623, -2.4134], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [94/100], Training Loss: 0.07014402002096176\n",
            "Output: tensor([-2.8409, -2.1812, -2.1124, -5.4528, -2.0053, -3.2638, -3.1343, -2.1150,\n",
            "        -3.9778, -2.4326], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [95/100], Training Loss: 0.06893999129533768\n",
            "Output: tensor([-2.8609, -2.1975, -2.1317, -5.4672, -2.0252, -3.2848, -3.1498, -2.1328,\n",
            "        -3.9931, -2.4517], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [96/100], Training Loss: 0.06776841729879379\n",
            "Output: tensor([-2.8806, -2.2135, -2.1507, -5.4815, -2.0448, -3.3056, -3.1649, -2.1505,\n",
            "        -4.0082, -2.4706], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [97/100], Training Loss: 0.06662774831056595\n",
            "Output: tensor([-2.9001, -2.2294, -2.1695, -5.4958, -2.0642, -3.3260, -3.1799, -2.1680,\n",
            "        -4.0233, -2.4894], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [98/100], Training Loss: 0.06551710516214371\n",
            "Output: tensor([-2.9193, -2.2450, -2.1880, -5.5099, -2.0835, -3.3463, -3.1949, -2.1854,\n",
            "        -4.0382, -2.5080], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [99/100], Training Loss: 0.06443535536527634\n",
            "Output: tensor([-2.9383, -2.2604, -2.2063, -5.5239, -2.1025, -3.3662, -3.2097, -2.2026,\n",
            "        -4.0529, -2.5264], grad_fn=<SliceBackward0>)\n",
            "Train labels: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Epoch [100/100], Training Loss: 0.06338164955377579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.size())\n",
        "print(train_labels_tensor.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djy7YopItqPA",
        "outputId": "705d268d-ceb6-4cc7-929e-e6bcaba6cce9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([676])\n",
            "torch.Size([676])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "model.eval()\n",
        "\n",
        "# Forward pass to generate predictions for all nodes\n",
        "with torch.no_grad():\n",
        "    predictions = model(data)\n",
        "    predictions = torch.sigmoid(predictions)  # Apply sigmoid activation for probability scores\n",
        "\n",
        "# Convert predictions to a numpy array for easier sorting\n",
        "predictions = predictions.cpu().numpy()\n",
        "\n",
        "# Sort predictions in descending order\n",
        "sorted_indices = np.argsort(predictions[:, 0])[::-1]  # Sort by the first column (probability of being similar)\n",
        "\n",
        "# Ensure that sorted_indices do not exceed the length of data.y\n",
        "sorted_indices = sorted_indices[sorted_indices < len(data.y)]\n",
        "\n",
        "# Get the top 5 authors (if data.y has at least 5 elements)\n",
        "if len(data.y) >= 5:\n",
        "    num_top_authors = min(5, len(sorted_indices))  # Ensure we don't go beyond the length of sorted_indices\n",
        "    top_authors_indices = sorted_indices[:num_top_authors]\n",
        "    top_5_authors = [data.y[i] for i in top_authors_indices]\n",
        "\n",
        "    # Print the top 5 authors\n",
        "    print(\"Top 5 Authors Who Could Co-Author:\")\n",
        "    for author in top_5_authors:\n",
        "        print(author)\n",
        "else:\n",
        "    print(\"Not enough authors in data.y to predict the top 5.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HJ-BpP0CvWa",
        "outputId": "64f3cc12-de0f-4ef1-b356-164a986e90f1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Authors Who Could Co-Author:\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "om6otdbkwLnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to map numerical indices to author_ids\n",
        "index_to_author_id = {index: author_id for author_id, index in author_id_to_index.items()}\n",
        "\n",
        "# Get the top 5 authors (if data.y has at least 5 elements)\n",
        "if len(data.y) >= 5:\n",
        "    num_top_authors = min(5, len(sorted_indices))  # Ensure we don't go beyond the length of sorted_indices\n",
        "    top_authors_indices = sorted_indices[:num_top_authors]\n",
        "    top_5_authors = [index_to_author_id[i] for i in top_authors_indices]\n",
        "\n",
        "    # Print the top 5 authors and their author_ids\n",
        "    print(\"Top 5 Authors Who Could Co-Author:\")\n",
        "    for author_index, author_id in zip(top_authors_indices, top_5_authors):\n",
        "        print(f\"Author Index: {author_index}, Author ID: {author_id}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"Not enough authors in data.y to predict the top 5.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbWRcZn2C83G",
        "outputId": "68c940fd-13e0-4190-f965-7c10126f6dd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Authors Who Could Co-Author:\n",
            "Author Index: 305, Author ID: (:Author {Feature140: \"0\",Feature146: \"0\",Feature145: \"0\",Feature148: \"0\",Feature147: \"0\",Feature90: \"0\",Feature142: \"0\",Feature141: \"0\",Feature92: \"0\",Feature144: \"0\",Feature143: \"0\",Feature91: \"0\",Feature94: \"0\",Feature93: \"0\",Feature96: \"0\",Feature95: \"0\",Feature98: \"0\",Feature139: \"0\",Feature138: \"0\",Feature97: \"0\",Feature99: \"0\",Feature151: \"0\",Feature150: \"0\",Feature157: \"0\",Feature156: \"0\",Feature159: \"0\",Feature158: \"0\",Feature153: \"0\",Feature152: \"0\",Feature81: \"0\",Feature155: \"0\",Feature80: \"0\",Feature154: \"0\",Feature83: \"0\",Feature82: \"0\",Feature85: \"0\",Feature84: \"0\",Feature87: \"0\",Feature149: \"0\",Feature86: \"0\",Feature89: \"0\",Feature88: \"0\",Feature124: \"0\",Feature123: \"0\",Feature126: \"0\",Feature125: \"0\",Feature120: \"0\",Feature70: \"0\",Feature122: \"0\",Feature121: \"0\",Feature72: \"0\",Feature71: \"0\",Feature74: \"0\",Feature73: \"0\",Feature117: \"0\",Feature76: \"0\",Feature75: \"0\",Feature116: \"0\",Feature119: \"0\",Feature78: \"0\",Feature77: \"0\",Feature118: \"0\",Feature79: \"1\",Feature135: \"0\",Feature134: \"0\",Feature137: \"0\",Feature136: \"0\",Feature131: \"0\",Feature130: \"0\",Feature133: \"0\",Feature132: \"0\",Feature61: \"0\",Feature60: \"0\",Feature63: \"0\",Feature62: \"0\",Feature65: \"0\",Feature128: \"1\",Feature64: \"0\",Feature127: \"0\",Feature67: \"0\",Feature66: \"0\",Feature129: \"0\",Feature69: \"0\",Feature68: \"0\",Feature102: \"0\",Feature223: \"0\",Feature222: \"0\",Feature101: \"0\",Feature104: \"0\",Feature103: \"0\",Feature224: \"0\",Feature100: \"0\",Feature221: \"0\",Feature220: \"0\",Feature219: \"0\",Feature216: \"0\",Feature215: \"0\",Feature218: \"0\",Feature217: \"0\",Feature5: \"0\",Feature4: \"0\",Feature3: \"0\",Feature2: \"0\",Feature1: \"0\",Feature9: \"0\",Feature8: \"0\",Feature7: \"0\",Feature6: \"0\",Feature113: \"0\",Feature112: \"0\",Feature115: \"0\",Feature114: \"0\",Feature111: \"0\",Feature110: \"0\",Feature109: \"0\",Feature106: \"0\",Feature105: \"0\",Feature108: \"0\",Feature107: \"0\",Feature201: \"0\",Feature200: \"0\",Feature203: \"0\",Feature202: \"0\",Feature212: \"0\",Feature211: \"0\",Feature214: \"0\",Feature213: \"0\",Feature210: \"0\",Feature209: \"0\",Feature208: \"0\",Feature205: \"0\",Feature204: \"0\",Feature207: \"0\",Feature206: \"0\",Feature10: \"0\",Feature12: \"0\",Feature11: \"0\",Feature14: \"0\",Feature13: \"0\",Feature16: \"0\",Feature15: \"0\",Feature18: \"0\",Feature17: \"0\",Feature19: \"0\",Feature182: \"0\",Feature181: \"0\",Feature184: \"0\",Feature183: \"0\",Feature180: \"0\",Feature189: \"0\",Feature186: \"0\",Feature185: \"0\",Feature188: \"0\",Feature187: \"0\",Feature50: \"0\",Feature52: \"0\",Feature51: \"0\",Feature54: \"0\",Feature53: \"0\",Feature56: \"0\",Feature55: \"0\",Feature58: \"0\",Feature57: \"0\",Feature59: \"0\",Feature193: \"0\",Feature192: \"0\",Feature195: \"0\",Feature194: \"0\",Feature191: \"0\",Feature190: \"0\",Feature197: \"0\",Feature196: \"0\",Feature199: \"0\",Feature198: \"0\",Feature41: \"0\",Feature40: \"0\",Feature43: \"0\",Feature42: \"0\",Feature45: \"0\",Feature44: \"0\",Feature47: \"0\",Feature46: \"0\",Feature49: \"0\",Feature48: \"0\",author_id: \"authorID_82c01_ce15b_431d4_20eb6_a1feb\",Feature160: \"0\",Feature162: \"0\",Feature161: \"0\",Feature168: \"0\",Feature167: \"0\",Feature169: \"0\",Feature164: \"0\",Feature163: \"0\",Feature166: \"0\",Feature165: \"0\",Feature30: \"0\",Feature32: \"0\",Feature31: \"0\",Feature34: \"0\",Feature33: \"0\",Feature36: \"0\",Feature35: \"0\",Feature38: \"0\",Feature37: \"0\",Feature39: \"0\",Feature171: \"0\",Feature170: \"0\",Feature173: \"0\",Feature172: \"0\",Feature179: \"0\",Feature178: \"0\",Feature175: \"0\",Feature174: \"0\",Feature177: \"0\",Feature176: \"0\",Feature21: \"0\",Feature20: \"0\",Feature23: \"0\",Feature22: \"0\",Feature25: \"0\",Feature24: \"0\",Feature27: \"0\",Feature26: \"0\",Feature29: \"0\",Feature28: \"0\"})\n",
            "Author Index: 293, Author ID: (:Author {Feature140: \"0\",Feature146: \"0\",Feature145: \"0\",Feature148: \"0\",Feature147: \"0\",Feature90: \"0\",Feature142: \"0\",Feature141: \"0\",Feature92: \"0\",Feature144: \"0\",Feature143: \"0\",Feature91: \"0\",Feature94: \"0\",Feature93: \"0\",Feature96: \"0\",Feature95: \"0\",Feature98: \"0\",Feature139: \"0\",Feature138: \"0\",Feature97: \"0\",Feature99: \"0\",Feature151: \"0\",Feature150: \"0\",Feature157: \"0\",Feature156: \"0\",Feature159: \"0\",Feature158: \"0\",Feature153: \"0\",Feature152: \"0\",Feature81: \"0\",Feature155: \"0\",Feature80: \"0\",Feature154: \"0\",Feature83: \"0\",Feature82: \"0\",Feature85: \"0\",Feature84: \"0\",Feature87: \"0\",Feature149: \"0\",Feature86: \"0\",Feature89: \"0\",Feature88: \"0\",Feature124: \"0\",Feature123: \"0\",Feature126: \"0\",Feature125: \"0\",Feature120: \"0\",Feature70: \"0\",Feature122: \"0\",Feature121: \"0\",Feature72: \"0\",Feature71: \"0\",Feature74: \"0\",Feature73: \"0\",Feature117: \"0\",Feature76: \"0\",Feature75: \"0\",Feature116: \"0\",Feature119: \"0\",Feature78: \"1\",Feature77: \"0\",Feature118: \"0\",Feature79: \"0\",Feature135: \"0\",Feature134: \"0\",Feature137: \"0\",Feature136: \"0\",Feature131: \"0\",Feature130: \"0\",Feature133: \"0\",Feature132: \"0\",Feature61: \"0\",Feature60: \"0\",Feature63: \"0\",Feature62: \"0\",Feature65: \"0\",Feature128: \"1\",Feature64: \"0\",Feature127: \"0\",Feature67: \"0\",Feature66: \"0\",Feature129: \"0\",Feature69: \"0\",Feature68: \"0\",Feature102: \"0\",Feature223: \"0\",Feature222: \"0\",Feature101: \"0\",Feature104: \"0\",Feature103: \"0\",Feature224: \"0\",Feature100: \"0\",Feature221: \"0\",Feature220: \"0\",Feature219: \"0\",Feature216: \"0\",Feature215: \"0\",Feature218: \"0\",Feature217: \"0\",Feature5: \"0\",Feature4: \"0\",Feature3: \"0\",Feature2: \"0\",Feature1: \"0\",Feature9: \"0\",Feature8: \"0\",Feature7: \"0\",Feature6: \"0\",Feature113: \"0\",Feature112: \"0\",Feature115: \"0\",Feature114: \"0\",Feature111: \"0\",Feature110: \"0\",Feature109: \"0\",Feature106: \"0\",Feature105: \"0\",Feature108: \"0\",Feature107: \"0\",Feature201: \"0\",Feature200: \"0\",Feature203: \"0\",Feature202: \"0\",Feature212: \"0\",Feature211: \"0\",Feature214: \"0\",Feature213: \"0\",Feature210: \"0\",Feature209: \"0\",Feature208: \"0\",Feature205: \"0\",Feature204: \"0\",Feature207: \"0\",Feature206: \"0\",Feature10: \"0\",Feature12: \"0\",Feature11: \"0\",Feature14: \"0\",Feature13: \"0\",Feature16: \"0\",Feature15: \"0\",Feature18: \"0\",Feature17: \"0\",Feature19: \"0\",Feature182: \"0\",Feature181: \"0\",Feature184: \"0\",Feature183: \"0\",Feature180: \"0\",Feature189: \"0\",Feature186: \"0\",Feature185: \"0\",Feature188: \"0\",Feature187: \"0\",Feature50: \"0\",Feature52: \"0\",Feature51: \"0\",Feature54: \"0\",Feature53: \"0\",Feature56: \"0\",Feature55: \"0\",Feature58: \"0\",Feature57: \"0\",Feature59: \"0\",Feature193: \"0\",Feature192: \"0\",Feature195: \"0\",Feature194: \"0\",Feature191: \"0\",Feature190: \"0\",Feature197: \"0\",Feature196: \"0\",Feature199: \"0\",Feature198: \"0\",Feature41: \"0\",Feature40: \"0\",Feature43: \"0\",Feature42: \"0\",Feature45: \"0\",Feature44: \"0\",Feature47: \"0\",Feature46: \"0\",Feature49: \"0\",Feature48: \"0\",author_id: \"authorID_8ede6_b2634_3305e_05c3c_0029f\",Feature160: \"0\",Feature162: \"0\",Feature161: \"0\",Feature168: \"0\",Feature167: \"0\",Feature169: \"0\",Feature164: \"0\",Feature163: \"0\",Feature166: \"0\",Feature165: \"0\",Feature30: \"0\",Feature32: \"0\",Feature31: \"0\",Feature34: \"0\",Feature33: \"0\",Feature36: \"0\",Feature35: \"0\",Feature38: \"0\",Feature37: \"0\",Feature39: \"0\",Feature171: \"0\",Feature170: \"0\",Feature173: \"0\",Feature172: \"0\",Feature179: \"0\",Feature178: \"0\",Feature175: \"0\",Feature174: \"0\",Feature177: \"0\",Feature176: \"0\",Feature21: \"0\",Feature20: \"0\",Feature23: \"0\",Feature22: \"0\",Feature25: \"0\",Feature24: \"0\",Feature27: \"0\",Feature26: \"0\",Feature29: \"0\",Feature28: \"0\"})\n",
            "Author Index: 291, Author ID: (:Author {Feature140: \"0\",Feature146: \"0\",Feature145: \"0\",Feature148: \"0\",Feature147: \"0\",Feature90: \"0\",Feature142: \"0\",Feature141: \"0\",Feature92: \"0\",Feature144: \"0\",Feature143: \"0\",Feature91: \"0\",Feature94: \"0\",Feature93: \"1\",Feature96: \"0\",Feature95: \"0\",Feature98: \"0\",Feature139: \"0\",Feature138: \"0\",Feature97: \"0\",Feature99: \"0\",Feature151: \"0\",Feature150: \"0\",Feature157: \"0\",Feature156: \"0\",Feature159: \"0\",Feature158: \"0\",Feature153: \"0\",Feature152: \"0\",Feature81: \"0\",Feature155: \"0\",Feature80: \"0\",Feature154: \"0\",Feature83: \"0\",Feature82: \"0\",Feature85: \"0\",Feature84: \"0\",Feature87: \"0\",Feature149: \"0\",Feature86: \"0\",Feature89: \"0\",Feature88: \"0\",Feature124: \"0\",Feature123: \"0\",Feature126: \"0\",Feature125: \"0\",Feature120: \"0\",Feature70: \"0\",Feature122: \"0\",Feature121: \"0\",Feature72: \"0\",Feature71: \"0\",Feature74: \"0\",Feature73: \"0\",Feature117: \"0\",Feature76: \"0\",Feature75: \"0\",Feature116: \"0\",Feature119: \"0\",Feature78: \"1\",Feature77: \"0\",Feature118: \"0\",Feature79: \"0\",Feature135: \"0\",Feature134: \"0\",Feature137: \"0\",Feature136: \"0\",Feature131: \"0\",Feature130: \"0\",Feature133: \"0\",Feature132: \"0\",Feature61: \"0\",Feature60: \"0\",Feature63: \"0\",Feature62: \"0\",Feature65: \"0\",Feature128: \"1\",Feature64: \"0\",Feature127: \"0\",Feature67: \"0\",Feature66: \"0\",Feature129: \"0\",Feature69: \"0\",Feature68: \"1\",Feature102: \"0\",Feature223: \"0\",Feature222: \"0\",Feature101: \"0\",Feature104: \"0\",Feature103: \"0\",Feature224: \"0\",Feature100: \"0\",Feature221: \"0\",Feature220: \"0\",Feature219: \"1\",Feature216: \"1\",Feature215: \"0\",Feature218: \"0\",Feature217: \"0\",Feature5: \"0\",Feature4: \"0\",Feature3: \"0\",Feature2: \"0\",Feature1: \"0\",Feature9: \"0\",Feature8: \"0\",Feature7: \"0\",Feature6: \"0\",Feature113: \"0\",Feature112: \"0\",Feature115: \"0\",Feature114: \"0\",Feature111: \"0\",Feature110: \"0\",Feature109: \"0\",Feature106: \"0\",Feature105: \"0\",Feature108: \"0\",Feature107: \"0\",Feature201: \"0\",Feature200: \"0\",Feature203: \"0\",Feature202: \"0\",Feature212: \"0\",Feature211: \"0\",Feature214: \"0\",Feature213: \"0\",Feature210: \"0\",Feature209: \"0\",Feature208: \"0\",Feature205: \"0\",Feature204: \"0\",Feature207: \"0\",Feature206: \"0\",Feature10: \"0\",Feature12: \"0\",Feature11: \"0\",Feature14: \"0\",Feature13: \"0\",Feature16: \"0\",Feature15: \"0\",Feature18: \"0\",Feature17: \"0\",Feature19: \"0\",Feature182: \"0\",Feature181: \"0\",Feature184: \"0\",Feature183: \"0\",Feature180: \"0\",Feature189: \"0\",Feature186: \"0\",Feature185: \"0\",Feature188: \"0\",Feature187: \"0\",Feature50: \"0\",Feature52: \"0\",Feature51: \"0\",Feature54: \"1\",Feature53: \"0\",Feature56: \"1\",Feature55: \"0\",Feature58: \"0\",Feature57: \"0\",Feature59: \"0\",Feature193: \"0\",Feature192: \"0\",Feature195: \"0\",Feature194: \"0\",Feature191: \"0\",Feature190: \"0\",Feature197: \"0\",Feature196: \"0\",Feature199: \"0\",Feature198: \"0\",Feature41: \"0\",Feature40: \"0\",Feature43: \"0\",Feature42: \"0\",Feature45: \"0\",Feature44: \"0\",Feature47: \"0\",Feature46: \"0\",Feature49: \"0\",Feature48: \"0\",author_id: \"authorID_0e17d_aca5f_3e175_f448b_acace\",Feature160: \"0\",Feature162: \"0\",Feature161: \"1\",Feature168: \"0\",Feature167: \"0\",Feature169: \"0\",Feature164: \"0\",Feature163: \"0\",Feature166: \"0\",Feature165: \"0\",Feature30: \"0\",Feature32: \"1\",Feature31: \"0\",Feature34: \"0\",Feature33: \"0\",Feature36: \"0\",Feature35: \"0\",Feature38: \"0\",Feature37: \"0\",Feature39: \"0\",Feature171: \"0\",Feature170: \"0\",Feature173: \"0\",Feature172: \"0\",Feature179: \"0\",Feature178: \"0\",Feature175: \"0\",Feature174: \"0\",Feature177: \"0\",Feature176: \"0\",Feature21: \"0\",Feature20: \"0\",Feature23: \"0\",Feature22: \"0\",Feature25: \"0\",Feature24: \"0\",Feature27: \"0\",Feature26: \"0\",Feature29: \"0\",Feature28: \"0\"})\n",
            "Author Index: 319, Author ID: (:Author {Feature140: \"0\",Feature146: \"0\",Feature145: \"0\",Feature148: \"0\",Feature147: \"0\",Feature90: \"0\",Feature142: \"0\",Feature141: \"0\",Feature92: \"0\",Feature144: \"0\",Feature143: \"0\",Feature91: \"0\",Feature94: \"0\",Feature93: \"0\",Feature96: \"0\",Feature95: \"0\",Feature98: \"0\",Feature139: \"0\",Feature138: \"0\",Feature97: \"0\",Feature99: \"0\",Feature151: \"0\",Feature150: \"0\",Feature157: \"0\",Feature156: \"0\",Feature159: \"0\",Feature158: \"0\",Feature153: \"0\",Feature152: \"0\",Feature81: \"0\",Feature155: \"0\",Feature80: \"0\",Feature154: \"0\",Feature83: \"0\",Feature82: \"0\",Feature85: \"0\",Feature84: \"0\",Feature87: \"0\",Feature149: \"0\",Feature86: \"0\",Feature89: \"0\",Feature88: \"0\",Feature124: \"0\",Feature123: \"0\",Feature126: \"0\",Feature125: \"0\",Feature120: \"0\",Feature70: \"0\",Feature122: \"0\",Feature121: \"0\",Feature72: \"0\",Feature71: \"0\",Feature74: \"0\",Feature73: \"0\",Feature117: \"0\",Feature76: \"0\",Feature75: \"0\",Feature116: \"0\",Feature119: \"0\",Feature78: \"0\",Feature77: \"0\",Feature118: \"0\",Feature79: \"1\",Feature135: \"0\",Feature134: \"0\",Feature137: \"0\",Feature136: \"0\",Feature131: \"0\",Feature130: \"0\",Feature133: \"0\",Feature132: \"0\",Feature61: \"0\",Feature60: \"0\",Feature63: \"0\",Feature62: \"0\",Feature65: \"0\",Feature128: \"1\",Feature64: \"0\",Feature127: \"0\",Feature67: \"0\",Feature66: \"0\",Feature129: \"0\",Feature69: \"0\",Feature68: \"0\",Feature102: \"0\",Feature223: \"0\",Feature222: \"0\",Feature101: \"0\",Feature104: \"0\",Feature103: \"0\",Feature224: \"0\",Feature100: \"0\",Feature221: \"0\",Feature220: \"0\",Feature219: \"0\",Feature216: \"0\",Feature215: \"0\",Feature218: \"0\",Feature217: \"0\",Feature5: \"0\",Feature4: \"0\",Feature3: \"0\",Feature2: \"0\",Feature1: \"0\",Feature9: \"0\",Feature8: \"0\",Feature7: \"0\",Feature6: \"0\",Feature113: \"0\",Feature112: \"0\",Feature115: \"0\",Feature114: \"0\",Feature111: \"0\",Feature110: \"0\",Feature109: \"0\",Feature106: \"0\",Feature105: \"0\",Feature108: \"0\",Feature107: \"0\",Feature201: \"0\",Feature200: \"0\",Feature203: \"0\",Feature202: \"0\",Feature212: \"0\",Feature211: \"0\",Feature214: \"0\",Feature213: \"0\",Feature210: \"0\",Feature209: \"0\",Feature208: \"0\",Feature205: \"0\",Feature204: \"0\",Feature207: \"0\",Feature206: \"0\",Feature10: \"0\",Feature12: \"0\",Feature11: \"0\",Feature14: \"0\",Feature13: \"0\",Feature16: \"0\",Feature15: \"0\",Feature18: \"0\",Feature17: \"0\",Feature19: \"0\",Feature182: \"0\",Feature181: \"0\",Feature184: \"0\",Feature183: \"0\",Feature180: \"0\",Feature189: \"0\",Feature186: \"0\",Feature185: \"0\",Feature188: \"0\",Feature187: \"0\",Feature50: \"0\",Feature52: \"0\",Feature51: \"0\",Feature54: \"0\",Feature53: \"0\",Feature56: \"0\",Feature55: \"0\",Feature58: \"0\",Feature57: \"0\",Feature59: \"0\",Feature193: \"0\",Feature192: \"0\",Feature195: \"0\",Feature194: \"0\",Feature191: \"0\",Feature190: \"0\",Feature197: \"0\",Feature196: \"0\",Feature199: \"0\",Feature198: \"0\",Feature41: \"0\",Feature40: \"0\",Feature43: \"0\",Feature42: \"0\",Feature45: \"0\",Feature44: \"0\",Feature47: \"0\",Feature46: \"0\",Feature49: \"0\",Feature48: \"0\",author_id: \"authorID_090d3_859ff_6840b_2280f_4708c\",Feature160: \"0\",Feature162: \"0\",Feature161: \"0\",Feature168: \"0\",Feature167: \"0\",Feature169: \"0\",Feature164: \"0\",Feature163: \"0\",Feature166: \"0\",Feature165: \"0\",Feature30: \"0\",Feature32: \"0\",Feature31: \"0\",Feature34: \"0\",Feature33: \"0\",Feature36: \"0\",Feature35: \"0\",Feature38: \"0\",Feature37: \"0\",Feature39: \"0\",Feature171: \"0\",Feature170: \"0\",Feature173: \"0\",Feature172: \"0\",Feature179: \"0\",Feature178: \"0\",Feature175: \"0\",Feature174: \"0\",Feature177: \"0\",Feature176: \"0\",Feature21: \"0\",Feature20: \"0\",Feature23: \"0\",Feature22: \"0\",Feature25: \"0\",Feature24: \"0\",Feature27: \"0\",Feature26: \"0\",Feature29: \"0\",Feature28: \"0\"})\n",
            "Author Index: 345, Author ID: (:Author {Feature140: \"0\",Feature146: \"0\",Feature145: \"0\",Feature148: \"0\",Feature147: \"0\",Feature90: \"0\",Feature142: \"0\",Feature141: \"0\",Feature92: \"0\",Feature144: \"0\",Feature143: \"0\",Feature91: \"0\",Feature94: \"0\",Feature93: \"0\",Feature96: \"0\",Feature95: \"0\",Feature98: \"0\",Feature139: \"0\",Feature138: \"0\",Feature97: \"0\",Feature99: \"0\",Feature151: \"0\",Feature150: \"0\",Feature157: \"0\",Feature156: \"0\",Feature159: \"0\",Feature158: \"0\",Feature153: \"0\",Feature152: \"0\",Feature81: \"0\",Feature155: \"0\",Feature80: \"0\",Feature154: \"0\",Feature83: \"0\",Feature82: \"0\",Feature85: \"0\",Feature84: \"0\",Feature87: \"0\",Feature149: \"0\",Feature86: \"0\",Feature89: \"0\",Feature88: \"0\",Feature124: \"0\",Feature123: \"0\",Feature126: \"0\",Feature125: \"0\",Feature120: \"0\",Feature70: \"0\",Feature122: \"0\",Feature121: \"0\",Feature72: \"0\",Feature71: \"0\",Feature74: \"0\",Feature73: \"0\",Feature117: \"0\",Feature76: \"0\",Feature75: \"0\",Feature116: \"0\",Feature119: \"0\",Feature78: \"1\",Feature77: \"0\",Feature118: \"0\",Feature79: \"0\",Feature135: \"0\",Feature134: \"0\",Feature137: \"0\",Feature136: \"0\",Feature131: \"0\",Feature130: \"0\",Feature133: \"0\",Feature132: \"0\",Feature61: \"0\",Feature60: \"0\",Feature63: \"0\",Feature62: \"0\",Feature65: \"0\",Feature128: \"1\",Feature64: \"0\",Feature127: \"0\",Feature67: \"0\",Feature66: \"0\",Feature129: \"0\",Feature69: \"0\",Feature68: \"0\",Feature102: \"0\",Feature223: \"0\",Feature222: \"0\",Feature101: \"0\",Feature104: \"0\",Feature103: \"0\",Feature224: \"0\",Feature100: \"0\",Feature221: \"0\",Feature220: \"0\",Feature219: \"0\",Feature216: \"0\",Feature215: \"0\",Feature218: \"0\",Feature217: \"0\",Feature5: \"0\",Feature4: \"0\",Feature3: \"0\",Feature2: \"0\",Feature1: \"0\",Feature9: \"0\",Feature8: \"0\",Feature7: \"0\",Feature6: \"0\",Feature113: \"0\",Feature112: \"0\",Feature115: \"0\",Feature114: \"0\",Feature111: \"0\",Feature110: \"0\",Feature109: \"0\",Feature106: \"0\",Feature105: \"0\",Feature108: \"0\",Feature107: \"0\",Feature201: \"0\",Feature200: \"0\",Feature203: \"0\",Feature202: \"0\",Feature212: \"0\",Feature211: \"0\",Feature214: \"0\",Feature213: \"0\",Feature210: \"0\",Feature209: \"0\",Feature208: \"0\",Feature205: \"0\",Feature204: \"0\",Feature207: \"0\",Feature206: \"0\",Feature10: \"0\",Feature12: \"0\",Feature11: \"0\",Feature14: \"0\",Feature13: \"0\",Feature16: \"0\",Feature15: \"0\",Feature18: \"0\",Feature17: \"0\",Feature19: \"0\",Feature182: \"0\",Feature181: \"0\",Feature184: \"0\",Feature183: \"0\",Feature180: \"0\",Feature189: \"0\",Feature186: \"0\",Feature185: \"0\",Feature188: \"0\",Feature187: \"0\",Feature50: \"0\",Feature52: \"0\",Feature51: \"0\",Feature54: \"0\",Feature53: \"0\",Feature56: \"0\",Feature55: \"0\",Feature58: \"0\",Feature57: \"0\",Feature59: \"0\",Feature193: \"0\",Feature192: \"0\",Feature195: \"0\",Feature194: \"0\",Feature191: \"0\",Feature190: \"0\",Feature197: \"0\",Feature196: \"0\",Feature199: \"0\",Feature198: \"0\",Feature41: \"0\",Feature40: \"0\",Feature43: \"0\",Feature42: \"0\",Feature45: \"0\",Feature44: \"0\",Feature47: \"0\",Feature46: \"0\",Feature49: \"0\",Feature48: \"0\",author_id: \"authorID_6db6e_b4af1_e18ab_81d38_78e44\",Feature160: \"0\",Feature162: \"0\",Feature161: \"0\",Feature168: \"0\",Feature167: \"0\",Feature169: \"0\",Feature164: \"0\",Feature163: \"0\",Feature166: \"0\",Feature165: \"0\",Feature30: \"0\",Feature32: \"0\",Feature31: \"0\",Feature34: \"0\",Feature33: \"0\",Feature36: \"0\",Feature35: \"0\",Feature38: \"0\",Feature37: \"0\",Feature39: \"0\",Feature171: \"0\",Feature170: \"0\",Feature173: \"0\",Feature172: \"0\",Feature179: \"0\",Feature178: \"0\",Feature175: \"0\",Feature174: \"0\",Feature177: \"0\",Feature176: \"0\",Feature21: \"0\",Feature20: \"0\",Feature23: \"0\",Feature22: \"0\",Feature25: \"0\",Feature24: \"0\",Feature27: \"0\",Feature26: \"0\",Feature29: \"0\",Feature28: \"0\"})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if len(data.y) >= 5:\n",
        "    num_top_authors = min(5, len(top_authors_indices))\n",
        "    print(\"For author:\",author_x)\n",
        "\n",
        "    print(\"Top 5 Authors Who Could Co-Author:\")\n",
        "    for author_index in top_authors_indices:\n",
        "        if author_index < len(df):\n",
        "            author_id = df.loc[author_index, 'author_id']\n",
        "            print(f\"Author Index: {author_index}, Author ID: {author_id}\")\n",
        "        else:\n",
        "            print(f\"Author Index: {author_index}, Author ID: Not found in DataFrame\")\n",
        "else:\n",
        "    print(\"Not enough authors in data.y to predict the top 5.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmNiENKbEF0R",
        "outputId": "497ea987-17d2-48ea-e18e-0d902e1136b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For author: authorID_3c152_85c04_fff40_024bb_8714b\n",
            "Top 5 Authors Who Could Co-Author:\n",
            "Author Index: 305, Author ID: authorID_82c01_ce15b_431d4_20eb6_a1feb\n",
            "Author Index: 293, Author ID: authorID_8ede6_b2634_3305e_05c3c_0029f\n",
            "Author Index: 291, Author ID: authorID_0e17d_aca5f_3e175_f448b_acace\n",
            "Author Index: 319, Author ID: authorID_090d3_859ff_6840b_2280f_4708c\n",
            "Author Index: 345, Author ID: authorID_6db6e_b4af1_e18ab_81d38_78e44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "a6WImUFB-Bhr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('model.pth'))"
      ],
      "metadata": {
        "id": "hloBE24s_Nzq",
        "outputId": "8b89b5f5-7915-4634-88a3-14d731d8ef55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "1VpLwWY9_SqF"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}